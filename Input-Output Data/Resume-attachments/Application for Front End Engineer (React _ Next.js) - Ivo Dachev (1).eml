Here is the complete implementation plan in Markdown format, ready to be saved as `1builder_rag_playbook_v2.md`.

````markdown
# OneBuilder Master Knowledge Base — RAG Implementation Playbook (v2.0)

**Stack:** Google Drive + Gemini File Search + n8n + Kestra + MCP Server  
**Target:** Production-Ready Knowledge Base  
**Timeline:** 30 Days  
**Date:** November 17, 2025

---

## 1. Executive Summary

This playbook defines the execution strategy for the OneBuilder Media Marketing RAG platform. The system consolidates all business knowledge (SEO, revenue, lead gen, sales, etc.) into a single, queryable AI system using a "Drive-First" architecture.

**Core Philosophy:**
* **Source of Truth:** A single Google Drive folder ("OneBuilder Master Knowledge").
* [cite_start]**Intelligence:** Google Gemini File Search handles vectorization, storage, and retrieval[cite: 33].
* **Orchestration:** Kestra handles the state and synchronization; n8n handles the real-time API gateways.
* **Control:** An MCP Server exposes the system to AI agents in VS Code (Augment) and ChatGPT.

---

## 2. Target Architecture

### 2.1 The Stack
1.  **Storage:** Google Drive (Raw files: PDF, Docx, Txt).
2.  **RAG Engine:** **Gemini File Search API** (Managed vector store, chunking, retrieval).
3.  **Orchestration:**
    * **n8n:** Real-time "micro-services" (`/query`, `/add-source`).
    * [cite_start]**Kestra:** Batch control plane (Drive sync, evaluation, error handling)[cite: 11].
4.  [cite_start]**Interface:** **MCP Server** (Official Kestra + Custom Facade) allowing AI agents to control the system[cite: 94].

### 2.2 Data Flow (The "Drive-First" Strategy)
1.  **Capture:** User drops a file (e.g., `q4-ppc-sacramento.pdf`) into the **"OneBuilder Master Knowledge"** Google Drive folder.
2.  **Sync (Kestra):** A scheduled Kestra flow (hourly) detects the new or updated file via a delta script.
3.  **Ingest (n8n):** Kestra triggers the n8n `/add-source` webhook.
    * n8n fetches the file.
    * n8n uses Gemini to extract metadata (vertical, geo, intent).
    * n8n uploads the file to Gemini File Search with `customMetadata`.
4.  **Query (User):** User asks a question via VS Code or ChatGPT.
5.  **Retrieve (n8n):** The Router (Gemini) converts the question into filters (e.g., `vertical="PPC"`). [cite_start]File Search retrieves relevant chunks using these filters[cite: 36].
6.  **Answer:** Gemini synthesizes an answer with citations.

---

## 3. Phase 1: Foundation & API Setup (Days 1–4)

**Goal:** Establish the cloud environment and the managed vector store.

1.  **Google Cloud Project:**
    * Enable **Gemini API** (`generativelanguage.googleapis.com`).
    * Create a **Service Account** and download the JSON key.
    * [cite_start]*Security Note:* Store this key in your local `.env` and GitHub Secrets[cite: 107].

2.  **Google Drive Setup:**
    * Create the root folder: **"OneBuilder Master Knowledge"**.
    * Create an error handling folder: **"FAILED_INGEST"**.
    * *Note:* Do not create sub-folders for categories. Categorization is handled strictly by metadata tags.

3.  **Create File Search Store:**
    * Using VS Code/Python, run a script to initialize the store.
    * **Store Name:** `onebuilder-master-kb`

4.  **Seed Gold Set:**
    * [cite_start]Upload 10–30 canonical "Gold Set" documents to the Drive folder[cite: 117].
    * Manually ingest them via script to test the connection, explicitly ensuring `customMetadata` (e.g., `vertical`, `geo`) is attached correctly.

---

## 4. Phase 2: Core Workflows (n8n) (Days 5–10)

[cite_start]**Goal:** Build the real-time ingestion and query "micro-services"[cite: 83].

### 4.1 Workflow: `/add-source`
* **Trigger:** Webhook (POST).
* **Inputs:** `url`, `manual_tags` (optional), `store_file_id_to_delete` (optional).
* **Logic:**
    1.  **Update Check:** If `store_file_id_to_delete` is present, use an HTTP Request node to call Gemini `files.delete` to remove the old version.
    2.  **Download:** Fetch file binary from `url`.
    3.  **Metadata Enrichment (The "Smart" Step):**
        * If `manual_tags` are missing, send the text to Gemini (`generateContent`).
        * *System Prompt:* "Extract the `vertical` (e.g., SEO, PPC), `horizontal` (e.g., lead-gen), and `geo` from this text. Return JSON."
    4.  **Consolidate:** Merge tags. **Crucial:** Add `drive_modified_time`, `drive_id`, and `filename` to the metadata.
    5.  **Upload:** Call Gemini `uploadToFileSearchStore` with the binary and `customMetadata`.
    6.  **Respond:** Return `{ "status": "success", "id": "..." }`.

### 4.2 Workflow: `/query`
* [cite_start]**Trigger:** Webhook (POST)[cite: 128].
* **Inputs:** `query` (text), `filters` (optional JSON).
* **Logic:**
    1.  [cite_start]**Router (Gemini):** "You are a router. Convert this user query into metadata filters: {vertical, horizontal, geo, intent}. Return JSON."[cite: 124].
    2.  **Filter Builder:** Convert JSON (e.g., `{geo: "Sacramento"}`) to the API filter string (`geo = "Sacramento"`).
    3.  **RAG Call:** Call Gemini `generateContent`.
        * **Tool:** `fileSearch` pointing to `onebuilder-master-kb`.
        * **Filter:** The string from step 2.
    4.  **Response:** Return `{ "answer": "...", "citations": [...] }`.

---

## 5. Phase 3: Automation & Scale (Kestra) (Days 11–18)

[cite_start]**Goal:** Automate synchronization and ensure data consistency[cite: 87].

### 5.1 Flow: `driveToFileSearchSync`
* **Schedule:** Hourly (`0 * * * *`).
* **Architecture:**
    1.  **List Drive:** Get all files in "OneBuilder Master Knowledge".
    2.  **List Store:** Get all files in the Gemini Store.
    3.  **Script (Python):** Compare lists to find **New** and **Updated** files.
    4.  **Loop (New):** Trigger n8n `/add-source` for each new file.
        * *OnError:* Move file to "FAILED_INGEST" folder & Slack alert.
    5.  **Loop (Updated):** Trigger n8n `/add-source` for updates (passing `store_file_id_to_delete`).

### 5.2 Python Script: `find_delta_and_updates.py`
This script is embedded in the Kestra `io.kestra.plugin.core.script.Python` task.

```python
import json, os
from datetime import datetime, timezone

def parse_iso(dt_str):
    if dt_str.endswith('Z'): dt_str = dt_str[:-1] + '+00:00'
    return datetime.fromisoformat(dt_str)

def get_mod_time(store_file):
    for m in store_file.get('customMetadata', []):
        if m['key'] == 'drive_modified_time': return parse_iso(m['stringValue'])
    return datetime.min.replace(tzinfo=timezone.utc)

# Load inputs
with open("drive_files.json") as f: drive_files = json.load(f)
with open("store_files.json") as f: store_files = json.load(f)

# Map Store Files
store_map = { f['displayName']: {'id': f['name'], 'time': get_mod_time(f)} for f in store_files if 'displayName' in f }

new_files = []
updated_files = []

for d_file in drive_files:
    d_name = d_file['name']
    d_time = parse_iso(d_file['modifiedTime'])
    
    if d_name not in store_map:
        new_files.append(d_file)
    elif d_time > store_map[d_name]['time']:
        updated_files.append({
            "drive_file": d_file,
            "store_file_id_to_delete": store_map[d_name]['id']
        })

# Output
with open(os.path.join(os.environ['KESTRA_OUTPUT_DIR'], 'output.json'), 'w') as f:
    json.dump({"new_files": new_files, "updated_files": updated_files}, f)
````

### 5.3 Flow: `handleDeletions`

  * **Schedule:** Daily.
  * **Logic:** Find files that exist in the *Store* but are missing from *Drive*. Delete them via API to prevent stale answers.

-----

## 6\. Phase 4: Integration (MCP Server) (Days 19–24)

[cite\_start]**Goal:** Allow AI Agents (VS Code, ChatGPT) to control the system[cite: 93].

### 6.1 Deploy Official Kestra MCP Server

Instead of building a custom server, we deploy the official Kestra MCP container.

1.  **Create Service Account:** In Kestra UI, create an account (`mcp-agent`) and generate an API Token.
2.  **Docker Command:**
    ```bash
    docker run -i --rm \
      -e KESTRA_BASE_URL="[http://host.docker.internal:8080](http://host.docker.internal:8080)" \
      -e KESTRA_API_TOKEN="<YOUR_TOKEN>" \
      ghcr.io/kestra-io/mcp-server-python:latest
    ```

### 6.2 Connect VS Code (Augment/Copilot)

Create `.vscode/mcp.json` in your workspace:

```json
{
  "servers": {
    "kestra": {
      "command": "docker",
      "args": [ 
        "run", "-i", "--rm", 
        "-e", "KESTRA_BASE_URL=[http://host.docker.internal:8080](http://host.docker.internal:8080)", 
        "-e", "KESTRA_API_TOKEN={{env:KESTRA_API_TOKEN}}", 
        "ghcr.io/kestra-io/mcp-server-python:latest" 
      ]
    }
  }
}
```

  * **Usage:** You can now ask Augment: *"Check the status of the last Drive Sync"* or *"Trigger the Ingestion flow."*

### 6.3 Custom MCP Facade (Optional)

To query the RAG system directly from VS Code (not just manage flows):

  * Build a simple Python MCP server.
  * **Tool:** `kb_ask(query)` -\> Calls n8n `/query`.
  * **Tool:** `kb_add_manual(url)` -\> Calls n8n `/add-source`.

-----

## 7\. Phase 5: Productionization (Days 25–30)

[cite\_start]**Goal:** Security, Evaluation, and Go-Live[cite: 100].

1.  **CI/CD (GitHub Actions):**

      * Deploy n8n workflows and Kestra flows to `staging`.
      * **Evaluation Gate:** A Kestra flow that runs the "Gold Set" queries against the n8n endpoint. It calculates **Hit@5** and **MRR**. [cite\_start]If scores drop, the deploy fails[cite: 91].
      * If Pass -\> Deploy to `prod`.

2.  **Security:**

      * **Secrets:** Move `GCP_SERVICE_ACCOUNT`, `N8N_TOKEN`, and `KESTRA_TOKEN` to GitHub Secrets and Kestra Secrets.
      * [cite\_start]**Network:** Whitelist Kestra IP in n8n settings[cite: 108].

3.  **Final Go-Live Test:**

      * Drop a "Sacramento PPC Strategy" PDF into Drive.
      * Wait for the Kestra sync.
      * Ask the Agent: "What is the PPC strategy for Sacramento?"
      * Verify answer and citation.

-----

## Appendix: API Contracts

**`POST /query` (n8n)**

```json
{
  "query": "How do I optimize for pSEO?",
  "filters": { "vertical": "SEO" }
}
```

**`POST /add-source` (n8n)**

```json
{
  "url": "[https://drive.google.com/uc?export=download&id=](https://drive.google.com/uc?export=download&id=)...",
  "type": "application/pdf",
  "store_file_id_to_delete": "files/12345", 
  "manual_tags": {
    "filename": "seo-guide.pdf",
    "drive_id": "123abc...",
    "drive_modified_time": "2025-11-16T10:00:00Z"
  }
}
```

```
```