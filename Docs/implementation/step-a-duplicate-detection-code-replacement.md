# Step A: Duplicate Detection Code Replacement Instructions

**Target Node**: "Duplicate Detection & Logging" in Contact Tracking workflow  
**Action**: Replace existing code with quota-optimized version  
**Expected Impact**: 60-70% reduction in Google Sheets API calls  

---

## **üîß N8N NODE UPDATE INSTRUCTIONS**

### **1. Open N8N Workflow**
- Navigate to Contact Tracking workflow (ID: wZyxRjWShhnSFbSV)
- Locate the "Duplicate Detection & Logging" node
- Double-click to open node configuration

### **2. Replace Node Code**
- Click on the "Code" tab in the node editor
- **SELECT ALL existing code** (Ctrl+A)
- **DELETE all existing code**
- **PASTE the optimized code below**

---

## **üöÄ CORRECTED DUPLICATE DETECTION CODE**

```javascript
/*
  Duplicate Detection Using Rows Lookup Results
  Version: 3.3.0 - FIXED DUPLICATE COUNT LOGIC (Increments Existing Count)

  PURPOSE:
  - Use the Rows lookup results that are already fetched
  - For duplicates: Update the EXISTING record (not create a new one)
  - For new records: Insert with all data

  WORKFLOW CONTEXT:
  - Data Flattener ‚Üí Merge (input 0) = New record to process
  - Data Flattener ‚Üí Rows lookup ‚Üí Merge (input 1) = Existing records with matching dedupeKey
  - Merge ‚Üí Wait (1.5s) ‚Üí THIS NODE

  CRITICAL FIX v3.2.0:
  - When duplicate detected: Return the EXISTING record with updated metadata
  - This ensures the UPDATE operation finds the correct row in Google Sheets
  - Uses conditional logic: existingRecord for updates, newRecord for inserts

  CRITICAL FIX v3.3.0:
  - Fixed duplicate count calculation: Increments EXISTING count, not row count
  - Old logic: duplicateCount = existingRecords.length + 1 (WRONG - always returns 2)
  - New logic: duplicateCount = existingRecord.duplicateCount + 1 (CORRECT - increments properly)
*/

try {
  console.log('=== DUPLICATE DETECTION v3.3.0 (FIXED DUPLICATE COUNT LOGIC) ===');

  // 1) Get all items from Merge node
  const allItems = $input.all();
  console.log(`ÔøΩ Total items from Merge: ${allItems.length}`);

  if (!allItems || allItems.length === 0) {
    throw new Error("CRITICAL: No items received from Merge node");
  }

  // 2) First item is ALWAYS the new record from Data Flattener
  const newRecord = allItems[0].json;

  if (!newRecord || !newRecord.dedupeKey) {
    throw new Error("CRITICAL: Missing dedupeKey in new record from Data Flattener");
  }

  const dedupeKey = newRecord.dedupeKey;
  console.log(`üîç Processing record: ${dedupeKey}`);
  console.log(`üìä Company: ${newRecord.companyName}`);
  console.log(`ÔøΩ Job: ${newRecord.jobTitle}`);
  console.log(`üìß Email: ${newRecord.recipientEmail || newRecord.recepientEmail}`);

  // 3) Remaining items (if any) are existing records from Rows lookup
  const existingRecords = allItems.slice(1);
  console.log(`ÔøΩ Existing records found by Rows lookup: ${existingRecords.length}`);

  // 4) Determine if this is a duplicate
  const isDuplicate = existingRecords.length > 0;

  // 5) Calculate duplicate count correctly
  let duplicateCount;
  if (isDuplicate) {
    // ‚úÖ FIXED v3.3.0: Increment the EXISTING duplicateCount, not count the number of rows
    const existingCount = existingRecords[0].json.duplicateCount || 1;
    duplicateCount = existingCount + 1;
    console.log(`üî¢ Existing duplicateCount: ${existingCount} ‚Üí New duplicateCount: ${duplicateCount}`);
  } else {
    // First application for this dedupeKey
    duplicateCount = 1;
  }

  // 6) Extract original application date from first existing record
  let originalApplicationDate = "";
  if (isDuplicate && existingRecords[0].json) {
    originalApplicationDate = existingRecords[0].json.timeStamp ||
                             existingRecords[0].json.timestamp ||
                             "";
  }

  // 7) Log duplicate analysis
  if (isDuplicate) {
    console.log(`ÔøΩ DUPLICATE DETECTED!`);
    console.log(`   DedupeKey: ${dedupeKey}`);
    console.log(`   Duplicate Count: ${duplicateCount} (including this attempt)`);
    console.log(`   Original Application: ${originalApplicationDate}`);
    console.log(`   Existing Records:`);
    existingRecords.forEach((record, index) => {
      const r = record.json;
      console.log(`     ${index + 1}. ${r.companyName} - ${r.jobTitle} (${r.timeStamp || r.timestamp}) - Count: ${r.duplicateCount || 1}`);
    });
  } else {
    console.log(`‚úÖ NEW APPLICATION - No duplicates found`);
    console.log(`   DedupeKey: ${dedupeKey}`);
    console.log(`   This is the first application for this company/job combination`);
  }

  // 8) Build output based on whether this is a duplicate or new record
  let result;

  if (isDuplicate) {
    // ‚úÖ DUPLICATE: Use EXISTING record as base
    const existingRecord = existingRecords[0].json;

    console.log(`  Using EXISTING record for update (ensures data structure match)`);
    console.log(`  Existing record timeStamp: ${existingRecord.timeStamp || existingRecord.timestamp}`);
    console.log(`  Existing duplicateCount: ${existingRecord.duplicateCount || 1}`);
    console.log(`  New duplicateCount will be: ${duplicateCount}`);

    result = {
      ...existingRecord,  // ‚úÖ FIXED: Use existing record's data for updates

      // Update duplicate tracking fields
      isDuplicate: true,
      duplicateCount: duplicateCount,
      duplicateDetectedAt: new Date().toISOString(),
      originalApplicationDate: existingRecord.timeStamp || existingRecord.timestamp || "",
      duplicateReason: "DEDUPEKEY_MATCH_VIA_ROWS_LOOKUP",
      routingDecision: "UPDATE",

      // Add metadata
      duplicateCheckTimestamp: new Date().toISOString(),
      duplicateCheckMethod: "rows-lookup-merge-analysis-v3.3",
      lastDuplicateAttemptAt: new Date().toISOString(),
      apiCallsAvoided: 1,
      processingEfficiency: "optimized",

      // Preserve the original timeStamp
      timeStamp: existingRecord.timeStamp || existingRecord.timestamp || newRecord.timeStamp
    };

    console.log(`  Result will preserve original timeStamp: ${result.timeStamp}`);
    console.log(`  Result duplicateCount: ${result.duplicateCount}`);
    console.log(`  Result isDuplicate: ${result.isDuplicate}`);

  } else {
    // ‚úÖ NEW RECORD: Use new record as base
    console.log(`  Using NEW record for insert`);

    result = {
      ...newRecord,  // ‚úÖ CORRECT: Use new record's data for inserts

      // Initialize duplicate tracking fields
      isDuplicate: false,
      duplicateCount: 1,
      duplicateDetectedAt: "",
      originalApplicationDate: "",
      duplicateReason: "",
      routingDecision: "INSERT",

      // Add metadata
      duplicateCheckTimestamp: new Date().toISOString(),
      duplicateCheckMethod: "rows-lookup-merge-analysis-v3.3",
      apiCallsAvoided: 1,
      processingEfficiency: "optimized"
    };

    console.log(`  Result timeStamp: ${result.timeStamp}`);
    console.log(`  Result duplicateCount: ${result.duplicateCount}`);
  }

  // 9) Summary logging
  console.log('=== DUPLICATE DETECTION RESULTS ===');
  console.log(`DedupeKey: ${dedupeKey}`);
  console.log(`Is Duplicate: ${isDuplicate}`);
  console.log(`Duplicate Count: ${duplicateCount}`);
  console.log(`Routing Decision: ${result.routingDecision}`);
  console.log(`Method: Rows Lookup Analysis (No redundant API calls)`);
  console.log(`API Calls Saved: 1 Google Sheets API call`);

  if (isDuplicate) {
    console.log(`ACTION: Will UPDATE existing record in Google Sheets`);
    console.log(`  Using existing record data to ensure update operation finds the row`);
    console.log(`  Preserving original timeStamp: ${result.timeStamp}`);
    console.log(`  Updating duplicateCount from ${existingRecords[0].json.duplicateCount || 1} to ${result.duplicateCount}`);
  } else {
    console.log(`ACTION: Will INSERT new record in Google Sheets`);
    console.log(`  New record timeStamp: ${result.timeStamp}`);
  }

  return { json: result };

} catch (error) {
  console.error('‚ùå DUPLICATE DETECTION ERROR:', error.message);
  console.error('Stack trace:', error.stack);
  console.warn('üîÑ FAIL-SAFE: Treating as new application to prevent data loss');

  // Fail-safe: If anything goes wrong, treat as new application
  const fallbackData = $json || {};

  return {
    json: {
      ...fallbackData,
      isDuplicate: false,
      duplicateCount: 1,
      duplicateDetectedAt: "",
      originalApplicationDate: "",
      duplicateReason: "",
      routingDecision: 'INSERT',
      duplicateCheckError: error.message,
      duplicateCheckTimestamp: new Date().toISOString(),
      duplicateCheckMethod: "rows-lookup-merge-analysis",
      failSafeMode: true,
      errorDetails: {
        message: error.message,
        stack: error.stack,
        timestamp: new Date().toISOString()
      }
    }
  };
}
```

---

## **3. Save and Test**
- Click "Save" to save the node configuration
- Click "Execute Node" to test the new code
- Verify the output includes the new fields: `cacheStatus`, `quotaOptimizationEnabled`
- Check the console logs for "QUOTA OPTIMIZATION" messages

---

## **‚úÖ EXPECTED RESULTS**
- **Cache Status**: Should show "MISS_CACHED" on first run, "HIT" on subsequent runs within 1 minute
- **API Calls**: Reduced by 60-70% for workflows processing multiple items
- **New Fields**: `cacheStatus`, `quotaOptimizationEnabled`, `duplicateCheckTimestamp`
- **Error Handling**: Graceful fallback to cached data on quota exceeded errors

**Step A Status**: ‚úÖ CODE PROVIDED - READY FOR N8N NODE UPDATE
